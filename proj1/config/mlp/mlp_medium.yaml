
# Training hyperparameters
batch_size: 64
lr: 0.001
epochs: 50
patience: 20
delta: 0.001
seed: 42
device: "cuda"
num_workers: 2
resume: false
save_model: false

# Dataset configuration
dataset:
  name: "mnist"
  augment: "simple"

# Model configuration
model:
  name: "mlp"
  params:
    layer_sizes: [512, 256, 128, 64]
    activation: "relu"
    dropout_rate: 0.0
    batch_norm: false
    name: "mlp_medium4"

# Weights & Biases configuration
wandb:
  enabled: true
  project: "deep-learning-application"
  entity: "martina-buccioni98-unifi"
  run_name: "mlp_medium4_mnist"
  log_images_every: 100

# Model saving
save_dir: "proj1/models/saved/mlp_medium4"