
- MLP
    - analizzato vanishing gradient 
    - analizzato deep vs medio vs shallow
    - far vedere le differenze tra i vari tipi di regolarizzazione
            -dropout aggressivo, medio e basso
            - batch norm
            - agumentation
-CNN
    - agumentation as regularization(nope)
    - deeper non vuol dire meglio (oki)
    - skip connections migliorano
    - double descent

- grad_cam
    -normale
    -fgsm attack
        interessanti esempi 4, 1, 7



    