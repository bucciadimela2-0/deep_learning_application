
A deep learning project implementing adversarial attack methods, out-of-distribution detection, and model robustness evaluation with comprehensive visualization tools.

<details>
<summary>ðŸŽ¯ Objectives</summary>

The project focuses on understanding model vulnerabilities and robustness through adversarial attacks and out-of-distribution detection:

- **Adversarial Attacks**
  - Implement **FGSM (Fast Gradient Sign Method)** attacks
  - Develop **PGD (Projected Gradient Descent)** attacks  
  - Create **few-pixel attacks** for sparse perturbations
  - Apply **genetic algorithm-based** adversarial generation

- **Out-of-Distribution Detection**
  - Evaluate model behavior on **unseen data distributions**
  - Compare **CNN vs Autoencoder** approaches for anomaly detection
  - Generate **ROC curves** and performance metrics

- **Model Analysis & Visualization**
  - Create comprehensive **attack visualizations**
  - Generate **confusion matrices** and performance plots
  - Analyze **score distributions** for normal vs anomalous data

</details>

> **Note:** This project explores the intersection of adversarial machine learning and anomaly detection, providing tools for comprehensive model robustness evaluation.

<details>
<summary>ðŸ“‚ Project Structure</summary>

```
proj4/
â”œâ”€â”€ attacks/                         # Adversarial attack implementations
â”‚   â”œâ”€â”€ __init__.py                  # Attack method exports
â”‚   â”œâ”€â”€ few_pixel.py                 # Sparse pixel-based attacks
â”‚   â”œâ”€â”€ fgsm.py                      # Fast Gradient Sign Method
â”‚   â”œâ”€â”€ genetic_attack.py            # Genetic algorithm attacks
â”‚   â””â”€â”€ pgd.py                       # Projected Gradient Descent
â”‚
â”œâ”€â”€ config/                          # Configuration files
â”‚   â”œâ”€â”€ adv_attack/                  # Adversarial attack configs
â”‚   â””â”€â”€ ood/                         # OOD detection configs
â”‚
â”œâ”€â”€ models/                          # Model architectures
â”‚   â”œâ”€â”€ __init__.py                  # Model factory
â”‚   â”œâ”€â”€ autoencoder.py               # Autoencoder for anomaly detection
â”‚   â””â”€â”€ cnn.py                       # CNN classifier
â”‚
â”œâ”€â”€ output_adv/                      # Adversarial attack results
â”œâ”€â”€ output_ood/                      # OOD detection results
â”‚
â”œâ”€â”€ utils/                           # Utility functions
â”‚   â”œâ”€â”€ data_utils.py                # Data loading and preprocessing
â”‚   â”œâ”€â”€ ood_eval.py                  # OOD evaluation metrics
â”‚   â””â”€â”€ plot_utils.py                # Visualization tools
â”‚
â”œâ”€â”€ main_adv.py                      # Adversarial attack orchestration
â””â”€â”€ main_ood.py                      # OOD detection experiments
 
```

</details>


<details>
<summary> Core Components â€“ Adversarial Attacks</summary>

This project includes implementations of several adversarial attack methods for neural networks.

FGSM (Fast Gradient Sign Method)
- Single-step attack using the gradient sign:  
  `x_adv = x + Îµ * sign(âˆ‡_x J(Î¸,x,y))`
  > *[Explaining and Harnessing Adversarial Examples]*  
  Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy, ICLR 2015

 PGD (Projected Gradient Descent)
- Multi-step FGSM with projection into an Îµ-ball for stronger attacks
 > *[Towards Deep Learning Models Resistant to Adversarial Attacks]* 
 >Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu, ICLR 2018

#Few-Pixel Attack
- Sparse perturbations targeting only high-gradient pixels
 >*[One Pixel Attack for Fooling Deep Neural Networks]*  
 > Jiawei Su, Danilo Vasconcellos Vargas, Kouichi Sakurai, IEEE TEC 2019

 Genetic Algorithm Attack
- Evolutionary optimization of perturbations through selection and mutation
 >*[Reference: *Generating Natural Language Adversarial Examples]*
 > Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, Kai-Wei Chang, EMNLP 2018

</details>
